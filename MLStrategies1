Data

    ---> Train/Validation/Test Split

    ---> Train/Validation


            validation/dev/cross-hold data/test 

    ----> Splitting Mechanisms

        ---> 500 ----> 50:50 / 70:30

        ---> 10000 ---> 80:20 ---> 80:10:10

            1. Train Data 8000
            2. Validation 2000


        ---> 1000000 ----> 99:1 ---> 98:1:1

            1. Train Data 990000
            2. Validation 10000

    ---> You will Train ML Model

        ----> 2 Metrics --->
            1. Accuracy ==> how many number of examples are correctly predicted/total number of examples
            2. Loss ======> Sigma(1,m) (Y^-Y)^2/2
                1. Binary Cross Entropy -- Binary Classifier
                2. Categorical Cross Entropy --- Multi Class Classifier
                3. Mean Squared Error ---- Regressors


        Train Data & Validation Data

        ML Training Flow:
            1. ML Model ---> (Linear, Neural Network)
            2. Weights initialize (random weights)
            3. Forward Propagation
                1. ML Model will take Train data
            4. Gradient Descent (Backward Propagation)
                1. Update Weights
            5. With Updated Weights 
                1. ML Model will forward propagate with Validation/Train Data
                2. Loss -- Val Loss, Train Loss
                3. Accuracy -- Val Accuracy, Train Accuracy
            
            6. You will repeat the some iterations


            ---> Train Accuracy & Train Loss/Train Error
            ---> Validation Accuracy & Validation Loss/Validation Error


    ---> Bias and Variance

        Variance: Average variability in the model predictions for the given dataset--train/validation
        
                Train Loss is very less & Validation Loss is high ---> Variance

                ---> Overfitting the ML Model
                ---> Learning the noise

                ---> Undesirable State

        Bias: Error Between average model predictions and ground truth (Actual Y)

                Train Loss is high and Valdiation loss is very high ---> Bias

                ---> Underfitting the ML Model
                ---> Not even learning the train data properly

    ---> Loss ---> Checkopointing loss, Threshold loss, Optimal loss

        Train Loss ---> 1%
        Validation Loss ---> 15% 

            ----> Learning noise in the data. --- Variance
            ----> Over training
    

        
        Train Loss ---> 10%
        Validation ---> 10%

            ---> Bias
            ---> Underfitted
            ---> Not even learnt train data --- Bias



Strategies for above problems:

        ---> Bias ---->
                Underfitting

                ----> Neural Network more bigger & change architecture
                ----> Train More


        ---> Variance ---->
                ----> Learn Noise in data
                ----> Overfitting the data

                ----> New data
                ----> Regularization 
                        1. Regularization
                        2. Dropout


        ---> Bias Variance Tradeoff ---> Optimal place in Bias & Variance


Example1 : My Optimal Loss is 0.01 for my specific data and specific model architecture

    ---> Experiment1: 3% Train Losss, 5% Validation Loss ---> Bias ---> Underfit
    ---> Experiment2: 0.09 Train Loss, 3% Validation Loss ---> Variance ---> Overfitting
    ---> Experiment3: 0.05% Train Loss, 0.09% Validation Loss ---> Normal Desired State

            validation loss > train loss



Experiment1 --> Experiment2 ===> Train More Time -- disadvanatge ---> Overfitting
Experiment2 --> Experiment3 ===> Regularization


L2 - Regularization: Controlling ML model weights in such a way that the model will not overfit

        Loss = J(W,b) = 1/2m * Sigma(1,m)(ypred-yactual)^2 + lambda/2m* Sigma(L2Norm(W)) --- Single Layer & Single Neuron


L2 - Regularization: (Neural Networks)

        Loss = J(W,b) = 1/2m * Sigma(1,m)(Ypred-yactual)^2 + lambda/2m Sigma(1,L) ||W(L)||

                                                            (#L,#L-1)

                                                           + Sigma(1,#L) as i, Sigma(1,#L-1) as j then L2Norm(W(i,j))

Beacause of weight decaying your weights may become or tends to zero ---> Your overfiting problem will be solved.



Dropout: 30%

    ---Every iteration ---> it will drops some weights


        Number of layers in your neural network ===> L

            4th layer ==> d4 = np.random.randn(4,1)<0.7


----------------> Dropout drops some neurons in particular layer for each training iteration 

====> disadvanatge:
    1. Training Loss Increase



Normalization:


    Your input data should be in Normal form
    Scaling datalower valuesinto lower values (0,1) (-1,1), (-3,3)

    Input

    tf.keras.layers.Normalization()(Input)


    Input Data X = {X1,X2,X3,X4,....Xn}

    Mean of the X = Mue

    Normalizaed X = (Input X - Mue)/Standard Deciation of X


Batch Normalization:

     Normalizing intermediate layer's outputs
     ---> Covariate Shift
        ---> Milatary 
    

    Total number of layers = L

    lth layer --->
        p number of nodes

            Z = {Z1,Z2,Z3,Z4,.....,Zp}

        Mean of Z = Mue
        Variance of Z = row^2

        Z^ = (Z-Mue)/sqrt(vaiance + Epsilon)


Quantization:
    ---> Float 64
    ---> Float 16





Vanishing & Exploding Problem --->
 ----> When you have a very deep neural network ---> Due to the random weight initizalation
 some how Weight of particular layer L becomes an Identity function with Identity value more than 1 ---> Exploding Problem

 -------> This will incur a huge hike in the loss


    Example: lth layer Weights[1.5 0
                                0  1.5]

                Z = W.T * X



Y^(L) = W(L).T * A(L-1) 

Y^(L) = W(L).T * activation[W(L-1).T*A(L-2)]

activation as Identity function and bias as 0

Y^(L) = W(L).T *[W(L-1)*[W(L-2)*[W(L-3)*[........*W[1]*X]]]]


Y^(L) = W^L * X

Y^(L) = (1.1*1.1)^L * X

Y^(L) will have more value ---> Loss == abs(Y^-Y)  ----> Huge hike in loss ---> Exploding Problem


        ---> If your weights in above situation is < 1 and is and Identity matrix then Network will suffer from 
        Vanishing Problem

Y^(L) = (0.9*0.9)^L * X

        ----> Loss == abs(Y^-Y) ----> HUge Hike in loss ---> Vanishing Problem

Weight Initialization

---> initizalation of weights should be properly

        ===> Weights = np.random.randn([2,2]) * sqrt(1/number of nodes in layer l)

             Weights = np.random.randn([2,2]) * sqrt(1/number of nodes in previour of lth layer)

             Weights = np.random.randn([2,2]) * sqrt(2/number of nodes in lth layer + number of nodes in l-1th layer)





Activation Functions:



========> Neural Networks will add non linearity to the functions


Sigmoid Function -===>

Sigmoid ==> f(x) = {
    1/1+e^-x
}

ReLu ===> f(x) = {
    x if x>=0
    0 if x<0
}

Leaky ReLU ==> f(x) = {
    x if x>=0
    max(0.01(x),x)
}

TanH === f(x) = {
    e^x - e^-x/e^x + e^-x
}


F(W.T.X + b)



Optimzations:

Gradient Descent: -- Minimizing Problem

    ---> Batch Gradient Descent
    ---> Mini Batch Gradient Descent
    ---> Stochastic Gradient Descent


Batch Gradient Descent ===>

    One Iteration will take whole data as an input and only one backward Propagation

    --> Number Iterations ---> epoch

Mini Batch Gradient Descent ===>

    One Iteration will have some of the data(Mini batch) only backward propagation for the mini batch

    Epoch ===> If you complete whole train dataset through training phase ---> epoch 

    Iteration ===> couple of iterations will make you one epoch

Stochastic Gradient Descent ===>

    Stochastic == Probabilistic

    One example at a time ---> randomly 

    number of examples m -->
    what is the probability of picking 1 ==> 1/m


    One Iteration ==> One example and only one backward propagation



Batch ---> Limited 2000,3000 ===> structural examples
Mini Batch ---> Huge data ===> mini batches ===> MINI_BATCH_SIZE = # examples in each batch ===> power 2
Stochastic ---> Limited --> Will be good in case of train examples are uniform in distribution



        uniform distribited data:
            ---> Every example ===> This will not effect the loss or backward propagation much

        non uniform distributed data:
            ---> Every Example ==> different loss ---> you will update weights differently ---> next example loss again high



Optimization Algorithms

    ---> Gradient Descent-- Algo
    --- Other Types of it