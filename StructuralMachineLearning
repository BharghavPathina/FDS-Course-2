Structural Data:

Problem: 
    -- Ecommerse customer spent Analysis
    -- prediction Problem: Set of Features ---> Predicting something

    -- Linear Regression
        If Structural Data is Linear Data
        -- To Build a ML model ---> f(X) -- where X - set of all features X = {x1,x2,x3,....xn}
            f(X) will be linear line/hyper line

      -- Advantages: 
        1. Time Series Analysis
        2. Stock Market -- Reinforcement
        3. Forecasting/prediction
        4. Easy to implement
        5. Easy to inferencing
        6. Historical Data--->

    y = f(x) = mx+c


    Y = F(X)


    Xi = {x1,x2,x3,x4,....xn}

    ith example in dataset
    #features/#columns/#attributes = n
    #examples/#instances/#rows/#fields = m


    X0 ---> 1st example
    X0 ---> [x0,x1,x2,x3,.....,xn]

    shape X0 = (n,1)

    shape X = [X0,X1,X2,....Xm] 
        -----> n*m --- if we stack examples in vertical 
        -----> m*n --- if we stack example in horizontal


    y ---> scalar -- single number
    Y ===> m examples---> (y0,y1,y2,....ym) 

    shape Y = (m,1)
          Y = (1,m)

    

    Y = F(X) = P1 * X1 + P2 * X2 + ....... + Pm * Xm

    Y = F(X) = P1 * [x0,x1,x2,...xn] + P2 * [x0,x1....xn].....


    Weights--->


                ---->  (1,n) * (n,m) + (1,m)
                    ==  (1,m) + (1,m)
                   y == (1,m)


    Y(1,m) = W(1,n) * X(n,m) + B(1,m)


    Y = WX + B

    Y(1,m) = W(n,1).T * X(n,m) + B(1,m)

    Y = W.T * X + B




    W * X = [(w1*x00+w2*x01+w3*x02+w4*x03),(w1*x10+w2*x11+w3*x12+w4*x13),
            (w1*x20+w2*x21+w3*x22+w4*x23),(w1*x30+w2*x31+w3*x32*w4*x33)]

    shape of W * X --->(1,4) ---> (1,m) --- m #examples


    W * X = [([w1,w2,w3,w4] * [x00,x01,x02,x03]), ([w1,w2,w3,w4] * [x10,x11,x12,x13])
            ([w1,w2,w3,w4] * [x20,x21,x22,x23]), ([w1,w2,w3,w4] * [x30,x31,x32,x33])]

    W * X --> [W] * [X]
             [W] --> (1,4)
             [X] --> (4,1)
              --> (1,1)

    W * X --> 

        W = [w1,w2,w3,w4]

        X = [x0,x1,x2,x3]

        x0 = [x00,x01,x02,x03]
        x1 = [x10,x11,x12,x13]


    W * X = [(W * x0), (W*x1), (W*x2), (W*x3)]

    W * X = [W * [x0,x1,x2,x3]]

        ---> (1,n) * (n,m)

    W * X ----> [W * X]


    Y = W*X + B

            W*X ---> (1,m)
            B -----> 6 ->>> ([1,m])



    question --> I have a dataset with 15 features and 10200 examples then what will be my Weights shape in linear regression or single nueron?

    W ---> (1,15) --- (1,n)
    X ----> (15,10200) -- (n,m)
    B ----> (1, 10200) ---> (1,m) B value is a scalar -->  Scalar (10) will be broadcasted to the shape of (1,10200) & Vectorization




Numpy()


 ------>a = np.array([1,2,3,4])
        a.shape ---> (4,) --> (4,1) --> (n,1)


        Y = W.T * X +B

        Y = np.dot(W.T,X) + B = F(X) -- Model


Y = np.dot(W.T,X) + B

Y ---> labels of m examples
X ---> features of m examples


Dataset---

X0 -- [1,2,3,4] -------------- 18' -- y
X1 -- [2,21,14,5] ------------ 42' -- y
.
.
.
.
.
.
X1000 [0,1,0,0] --------------- 5' -- Y



Y = F(X)


f(x) = 2x^2 + 3x -10
-- Convex functions

----> Every Convex function will get a global minima
----> If you have n features ----> you will end up with n+1 dimesions
----> On top of n+1 dimesions you will be having a convex function always for linear regression
----> If any function is convex function then we will get global minima of it.
----> We are going to find that global minima of the function with the help of technique called Gradient Descent
----> gradient ---> slope ----> f(x) ---> d(f(x))/dx ---> 
                                ----> many parameters (W,B) ---> F(W,B) ---> dow(F(W,B))/d(W)---> slope of F(W,B) w.r.t W --- B will be constant       
                                                                        ---> dow(F(W,B))/d(b) ---> slope of F(W,B) w.r.t B --- W will be constant


----> You can achieve 100% accuracy with traindata


F(2) = 8 + 6 -10
F(2) = 4


F(X) =  Y = W.T * X + B




Particular combination of W,B will be a model ---> 





parameters ---> W, B ---> Weights, Biases







XNew == [20,0,20,0] ----------> F(X)
