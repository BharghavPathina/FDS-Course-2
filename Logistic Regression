ML Predictions:
    1. Regression  ----------------> The Output of the Prediction is continuous value/number
    2. Classification -------------> The Output of the Prediction is discrete value/


Logistic Regression: Its a Classification problem
    ---> 2 classes/categories---> Binary Classification problem
    ---> more than 2 classes/categories---> Multi label classification problem


Linear Regression --> Single Neuron without activation
Logistic Regression --> Single Neuron with activation ---> Perceptron

y = mx+c

X --> [X0,X1,X2,...Xm] -- #examples = m
X(i) --> [x0,x1,x2,.....xn] -- #features = n

Y^ = np.dot(W.T,X) + b
Y^ = W.T * X + b

Shape of W ---> (1,n)
Shape of b ---> Scalar value
total number of parameters = n+1
Coordinate space dimensions = n+1

Classification problem ---> Binary Classification

Tranforming the Linear Regressor (Y^ = W.T * X + b) to a function which specifies (0,1)

Y^  =  W.T * X + b

One Tranforming function called Sigmoid Function ---> Activation function 

Sigmoid Function F(X) = 1/1+exp(-x)

x===> +inf

exp(-x) = 1/exp(x)
        = 1/exp(inf)

        Limit x>inf 1/x = 0

        = 0
F(X) = 1/1+0 = 1

x ===> -inf

exp(-x) = exp(-(-inf)) = exp(inf) ==large

F(X) = 1/1+large
     = 1/large

     = 0

Z = W.T * X + b

Y^ = Sigmoid(Z) = 1/1+exp(-Z) = 1/1+exp(-(W.T*X+b)) -- Logistic Regression

Y^ = Sigmoid(np.dot(W.T,X)+b)

Y^ = 1/1+exp(-(W.T*X+b))

Y^ = 1/1+exp(-Z)


Loss/Cost Function ---> Sigma(1,m)(Y^(i)-Y(i))^2/2 -- (Mean Squared Error)

-----------------------> Sigma(1,m)(Sigmoid((W.T*X(i)+b)),Y(i))^2/2

    --------------------> We wont get convex function --> Not applicable for Gradient Descent

    -----------------> Loss Function (Log Loss) ---> J(W,b) = i overs from 1,m: -(Y(i)log(Y^(i))+(1-Y(i))log(1-Y^(i))) is a convex function

                    -----> Y^ --- (0,1) --> Threshold ---> Y^ is more than 0.8 then Y^ is 1 else 0

                    -----> Actual Y ---> 1

                    J(W,b) = -(YlogY^)
                    J(W,b) = -(logY^)
                    J(W,b) = -(logY^) ---> Never becomes 0

                    J(W,b) = -log(Y^) ---> Y^ tends to 0

                    limit Y^->0 will give me J(W,b) = -(log(Y^)) = 0

                    --- Negative example ===========> -(-inf) = large number---> 


                    -----> Actual Y ---> 0

                    J(w,b) = -((1-Y)log(1-Y^))
                    J(W,b) = -(log(1-Y^))

                    J(W,b) ---> 0 (minimal loss)

                    (1-Y^) ----> 1
                    Y^ ------> 0
                    --- Negative example ===========> -(log(1-Y^)) = -(log(0)) ==> large number

                    Lmt x->0 log(x) == -inf


    ---------------------> We can apply gradient descent --> best minimum possible values for W, b so that the loss will low..


    Cost Function = J(W,b) = L = -(ylog(y^)+(1-y)log(1-y^))

    Z = W.T * X + b
    A = Y^ = Sigmoid(Z) = 1/1+exp(-Z)
    L(A,Y) = -(Y(log(A)+(1-Y)log(1-A)))

Gradient Descent Algorithm -- Basic
Training Phase:
1. Random initilization of W,b
2. find out the cost function --> Training dataset ---> (Ypred - Yactual)
3. slope of that function w.r.t both
---> Until you get the minima---> (W,b) are not changing ---> Converging 
4. Update W,b:
    W(Next) = W(Prev) - alpha*(slope(J(W,b)) w.r.t W)
    b(Next) = b(Prev) - alpha*(slope(J(w,b)) w.r.t b)



slope of L(A,Y) w.r.t W --->
slope of L(A,Y) w.r.t b --->



Forward Propagation:

BackPropagation in Gradient Descent:
    ---> Finding derivativs of L(A,Y) wr.t W,b

    Derivative of L w.r.t A =  dA = P(L(A,Y))/dA 
    Derivating of L wr.t Z = dZ = P(L(A,Y))/dZ = P(L(A,Y))/dA * P(A)/dZ = dA * P(A)/dZ
    Derivative of L w.r.t W1 = dW1 = P(L(A,Y))/dW1 = dZ * P(Z)/dW1
    Derivative of L w.r.t W2 = dW2 = P(L(A,Y))/dW2 = dZ * P(Z)/dW2
    Derivative of L w.r.t b = db = P(L(A,Y))/db = dZ * P(Z)/db


    dA = P(L(A,Y))/dA = P(-(Ylog(A)+(1-Y)log(1-A)))/dA

                        = -(Y * 1/A + (1-Y)1/(1-A)(-))
                        = -(Y/A - (1-Y)/(1-A))

    
    dZ = dA * P(A)/dZ

    P(A)/dZ = P(Sigmoid(Z))/dZ
            = P(1/1+exp(-z))/dZ
            = A(1-A)

    dZ = -(Y/A -(1-Y)/(1-A)) * A(1-A)
        = A-Y


    dW1 = dZ * P(Z)/dW1

    P(Z)/dW1 = P(W1X1+W2X2+b)/dW1
             = X1

    dW1 = (A-Y) * X1
    dW2 = (A-Y) * X2
    db = (A-Y) * 1


    alpha = 0.01

    W1Next = WPrev -(alpha)dW1
    W2Next = W2Prev -(alpga)dW2
    bNext = bPrev - (alpha)db


    











