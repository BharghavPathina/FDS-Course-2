1. LeNet 

    (32,32,1) --> Conv(f=5,s=1,#f=6) ---> (28,28,6) ---> AvgPool(f=2,s=2) ---> (14,14,6) ---> Conv(f=5,s=1,#f=16)
    (10,10,16) --> AvgPool(f=2,s=2) ---> (5,5,16) ---> Flatten ---> (400,) ---> Dense1(120) ---> Dense(84)


2. AlexNet
3. VGG16


---> ResNet: To solve vanishing and exploding problem in CNNs we have Residual Block

        ---> Residual Block is making skip connections between layers

                Layer1->Layer2->layer3->layer4

        Examples ==> ResNet50, ResNet152, ResNet101


---> Point Convolution (1,1) Convolution 


    (6,6) ---> f=1,s=1,p=0,#f=1 ---> (6,6,1)

            Number of parameters = (1,1,1) = 1

    
    (6,6,32) ---> f=1,s=1,p=0,#f=1 ---> (6,6,1)

                    f=(1,1,32) * 1

            Number of parameters = (1,1,32) *1 = 32



    
    (28,28,192) ----> (f=5,s=1,p=1,#f=32) ----> (28,28,32)

        Number of paramaters = (5,5,192) * 32 = paramaters

        Number of multpications = (5*5*192)*(28*28*32)



    (28,28,192) ----> (f=1,s=1,p=1, #f=32) ----> (28,28,32)

        Number of parameter =  (1,1,192) *32 =
        Number of multpications = (1*1*192) * (28*28*32)



Inception Net:

    (28,28,192) * different kernel/filter sizes operations same padding

    (28,28,192) 

    Stacking one after the other which will make the output as very huge channeled

    On a very channeled input we will do point Convolution so that we will end up with less number of parameters multpications



MobileNet:

    Depth-Wise-Seperable --- point convolutions

    Normal Conv
    (6,6,3) --> f=3,p=0,s=1, #f=16 --> (4,4,16)
        number of paramaters = (3,3,3) * 16 
        number of multiplications = (3*3*3) * (4*4*16) 

        Total number of mult = 6912

    Depth-Wise Seperable
    (6,6,3) 
        1. Depth Operation with less number filters
        2. Seperable Operation using point convolution with more number of filters

        (6,6,3) --> f=3,p=0,s=1, #f=3 --> (4,4,3)
            number of mult = (4*4*3) * (3*3*3) = 1296

        (4,4,3) --> f=1,p=1,s=1, #f=16 --> (4,4,16)
            number of mult = (4*4*16) * (1*1*3) = 768

        total number of mult = 2064

        2064/6912 = 30%


    Using Depth-Wise Seperable technique we will have a network called MobileNet


Efficient Net:

    Computaional constraints, accuracy


    will have three paramaters
        1. Resolution of the image
        2. Depth of the NN (#no of layers)
        3. Width of the NN (#no of nodes in layer)

    EfficientB1 -B7



Transfer Learning

    Batching ===> 

            GPU Memory = 8GB
            Availabale GPU Memory = 4GB

            Total number of examples = 1200
            1 image is around 600KB

            batch size of 32 ==> 1200/32 --->37 * 600KB = 22200KB
                                                        = 22.2MB


            1. Prefetch ---> pipeline
            2. Cache

        12GB


Data Augmentation:

    Augmentation Techniques:
        1. Rotate
        2. 




Transfer Learning Steps:


    1. Select Base Model
        "ImageNet" 
        1. MobileNetV1
        2. MobileNetV2
        3. ResNet101
        4. ResNet50
        5. ResNet152
        6. InceptionNetV2
        7. EfficientNetB1
        8. EfficientNetB7

        tf.keras.applications.mobile_net_v2.MobileNetV2(include_top=False)

        1. MobileNetV2 --> trainable = False
        2. fineTuning
            ---> First 100 layers trainable = False
            ---> Next 50 layers trainable = True

    2. Rescale our dataset

        1. (0-1) = 1/255
        2. (-1,1) = 1/127.5

    3. Augmentation (Optional)

        1. Rotate
        2. Flip
        3. Birghtness
        4. Contrast

    4. Specific base model ---> preprocess_input

        tf.keras.applications.mobile_net_v2.preprocess_input

    5. base_model

        1. Whole base model is not trainable
    
    6. Dense layers
        1. trainable -- weights will be updated
    7. Output Layer
        1. trainable -- weights will be updated


fineTuning Transfer learning:

        fineTuning Base model with the new custom data is called finetuning Transfer learning

    