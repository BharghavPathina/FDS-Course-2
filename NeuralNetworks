Linear Regression
Logistic Regression
Decision Trees
Support Vector Machines

Perceptron
Single Layer Neural Network
Multi Layer Neural Network
Deep Neural Networks


number of examples  -- m
number of features per example -- n
features/example -- x0,x1,x2,x3,..
features/particular examples (i) ---- xi0,xi1,xi2,xi3...

Perceptron

        Perceptron is a single neuron or node Network
        Neuron will have 2 parts
            --- Z function
            --- A (Activation Function)

        
        Training Phase --- To get best weights and bias for predicting
        Inference Phase --- To predict or infer in realtime using the trained Neural Network

        Forward Propagation & Backward Propagation
            --- Evry training phase will have both

        Inference phase will only have forward Propagation


        Forward Propagation: 
-- from input it will travarse through last node/neuron -- 



--- from Last node we have to come back to input layer to find all the wrights and biases of all neurons 



Single Layer Neural Network

--- 1. Input layer (only one)
    2. Hidden Layers (one or more)
    3. Output Layer (only one)



1. Input Layer: --- Shape of the input matrix -- (m,n)

2. Hidden Layers --- One Layer -- Number of nodes in particular layer -- p 

3. Output Layer --- Shape of the output matrix --  


    ---- X00,X01,X02,X03 -n
    ---- Z00,Z01,Z02,Z03 -p
    ---- A00,A01,A02,A03 -p

    Input Layer ====> (m,n)

    Hidden Layer ====> (n,p)

    output Layer ====> (p,1)


    (m,n) * (n,p) * (p,1) ==  (m,1) --- For each example you have one output



    Weights and Bias ===> 
(m,n)

    Weights and biases for each layer of the Network
        ==> Input Layer ===> weights = (n,1), bias = (1,) only for one node in a layer
        ==> Hidden Layer ===> weights =  (n,p) bias
        ==> Hidden Layer ===> bias = (1,p)
        ==> Output Layer ===> weights = (p,1)

    

    More Generalized Formulae ===> 

        Nodes ===> Node(00),Node(01),Node(02)

        Z and A

        Z == W.T * X + b

        A == Activation(Z)


        Every Node will have Z and A 
            
        Input of every node ==> will be outputs of previous layers nodes
        Output of every node ==> A


        First Layer ==> Input layer
            ===> A(00), A(01), A(02),A(03) = X(i0),X(i1),X(i2),X(i3)

        First Hidden Layer ==> Second Layer
            ===> A(10), A(11), A(12)

        Second Hidden Layer ==> Third Layer
            ===> A(20), A(21)

        Output Layer ==> Fourth Layer
            ===> A(30)


        First Hidden Layer
        Weights ===> W(L0) ==> (1,#(L-1))
        Weight at particular layer ===> W(L) ===> (#L,#(L-1))
    
        
        
        

        First Hidden Layer = Z(00),Z(01),Z(02),Z(03)

                Z(00) = W.T * A + b


                Z(L0)  = W 